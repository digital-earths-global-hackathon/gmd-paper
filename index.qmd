---
title: "km scales hacked at global scale"
author:
  - name: Florian Ziemen
  - name: Lukas Kluft
  - name: Tobias Kölling
  - name: Mark Mütz
  - name: Sam Greenfield
  - name: Andrew Gettelman
  - name: put your name here
format:
  html:
    toc: true
---
## Key points for this paper

* Pulled off a 600-person 10-location multi-model intercomparison with one year of run-up and close to no dedicated resources, using a tech stack that was new to n-1 teams.

* 10 nodes around the globe
  * different backgrounds / resources
  * same view on data
* X models, regional + global
  * standardized names (or provided as Zarr)
  * Output remapped to HEALPix
  * Data stored in Zarr with
  * Indexed with intake 0.7
* Common python base stack
  * Jupyterhub setup at several locations

## Abstract

## Introduction

* km-scale as a revolution in climate workflows
  * previously 100km scale, now 5 km scale
  * factor 20 horizontally yields 400 in data volume per record
  * new workflows necessary

* Paradigm shift towards opening data
  * traditional approach (netCDF / ...) copy data over in explicit file transfers, then access those
  * Network access via Zarr - only what you really need is transferred, and that without your intervention
* Data sharing as default
* Common catalog that merges all datasets available online
* previous intercomparisons
  * CMIP
  * DYAMOND Summer, Winter
    * Summer: come as you are, no standardization
    * Winter: worked our way through to netCDF, some standardization
* km-scale hackathons
  * nextGEMS, EERIE, ...

* Global hackathon: 
      * All remapped to HEALPix, 
      * zarr
      * Common catalog with online DS as backup in github
      * Shared environment spec for easy set-up of similar/identical workflow env

## Tech team
  * Some nodes have dedicated tech staff, sometimes the PIs took the role
  * Some previous contacts but no dedicated meeting prior to hackathon
  * Communication via Mattermost, Github, and a bit of zoom

## Tech stack

### Catalog
* Requirements
  * easy to load data
  * support for variants (time, zoom) in open call
  * same structure for all locations, so sharing code is easy
  * some support for multi-file datasets

* Market
  * Intake 0.7
  * Intake 2.0
  * STAC
* Outlook
  * Improve STAC
  * Work towards cleaner datasets

### Grid
* Requirements
  * Spatially coherent storage
  * Hierarchies
  * Easy access
  * Equal Area

* Market
  * Lat/Lon
  * ICON / ...
  * Gaussian reduced
  * HEALPix

### Data format
* Requirements
  * Open big datasets fast
  * Chunking in all dimensions
  * Works via network
* Market
  * netCDF (+Kerchunk)
  * Zarr
  * GRIB (+Kerchunk)
