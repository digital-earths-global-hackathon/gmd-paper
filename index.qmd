---
title: "km scales hacked at global scale"
author:
  - name: Florian Ziemen
  - name: Lukas Kluft
  - name: Tobias Kölling
  - name: Mark Muetzefeld
  - name: Sam Greenfield
  - name: Andrew Gettelman
  - name: Fabian Wachsmann
  - name: put your name here
format:
  html:
    toc: true
---
## Key points for this paper

* Pulled off a 600-person 10-location multi-model intercomparison with one year of run-up and close to no dedicated resources, using a tech stack that was new to n-1 teams.

* 10 nodes around the globe
  * different backgrounds / resources
  * same view on data
* X models, regional + global
  * standardized names (or provided as Zarr)
  * Output remapped to HEALPix
  * Data stored in Zarr with
  * Indexed with intake 0.7
* Common python base stack
  * Jupyterhub setup at several locations

## Abstract

We present the results and technical achievements of a global, multi-model intercomparison hackathon involving over 600 participants across 10 locations. Despite minimal dedicated resources and a largely new technology stack for most teams, we successfully standardized and remapped outputs from a diverse set of regional and global climate models to a common HEALPix grid, storing the data in Zarr format and indexing it with Intake 0.7. This unified approach enabled all participating nodes—regardless of background or resources—to access and analyze the same datasets efficiently. A common Python base stack, deployed via JupyterHub at several sites, facilitated collaborative analysis and reproducibility. Our experience demonstrates the feasibility and benefits of rapid, large-scale, distributed climate data analysis using modern open-source tools and cloud-ready data formats.


## Introduction

* km-scale as a revolution in climate workflows
  * previously 100km scale, now 5 km scale
  * factor 20 horizontally yields 400 in data volume per record
  * new workflows necessary

* Paradigm shift towards opening data
  * traditional approach (netCDF / ...) copy data over in explicit file transfers, then access those
  * Network access via Zarr - only what you really need is transferred, and that without your intervention
* Data sharing as default
* Common catalog that merges all datasets available online
* previous intercomparisons
  * CMIP
  * DYAMOND Summer, Winter
    * Summer: come as you are, no standardization
    * Winter: worked our way through to netCDF, some standardization
* km-scale hackathons
  * nextGEMS, EERIE, ...

* Global hackathon: 
      * All remapped to HEALPix, 
      * zarr
      * Common catalog with online DS as backup in github
      * Shared environment spec for easy set-up of similar/identical workflow env

## Tech team
  * Some nodes have dedicated tech staff, sometimes the PIs took the role
  * Some previous contacts but no dedicated meeting prior to hackathon
  * Communication via Mattermost, Github, and a bit of zoom

## Tech stack

### Catalog
* Requirements
  * easy to load data
  * support for variants (time, zoom) in open call
  * same structure for all locations, so sharing code is easy
  * some support for multi-file datasets
  * easy to handle

* Market
  * Intake 0.7 does all of this, but is outdated
  * Intake 2.0 does not do the variants, complex format
  * STAC does not do the opening, does not do the variants, does not like more than one file per entry (asset)
* Solution: Intake 0.7
* Outlook
  * Improve STAC
  * Work towards cleaner datasets

### Grid
* Requirements
  * Spatially coherent storage
  * Hierarchies
  * Easy access
  * Equal Area

* Market
  * Lat/Lon (not equal area, 50% data overhead compared to equal area for same res at eq)
  * ICON / ... usually no direct computations between lat/lon and index
  * Gaussian reduced no SFC
  * HEALPix Ticks all boxes.
* Solution: HEALPix, regional grids by storing indices with the data, and filling chunks with NaN around data
* Outlook:
  * Improve support for differential operators / ...

### Data format
* Requirements
  * Open big datasets fast
  * Chunking in all dimensions
  * Works via network
* Market
  * netCDF (+Kerchunk) Can be served as zarr via remote. Needs kerchunk to get fast opening of full DS, that's only supported in py
  * Zarr Ticks all boxes
  * GRIB (+Kerchunk) (all downsides of netCDF + miserable chunking)
* Solution: Mostly zarr, bits of GRIB hidden behind server, causing some trouble.
* Outlook:
  * Go to zarr3 to get rid of small files on disk, once more widely supported
  * Improve netCDF support for zarr3

### Python environment
* Requirements
  * Python stack that can open and do standard analysis on data
* Market
  * conda(forge) works, and includes pip support
  * pip needs a python
  * pixi is nicer, but less known, conda-(forge) based
* Solution: Conda recipe in github with some CI testing
* Outlook:
  * Consider pixi

### HPC access
* Requirements
  * Nodes with some compute power for data-intensive analyses
  * User-friendly scripting environment
  * Nodes with internet access for working with remote datasets
* Market
  * JPH has no requirements on user side, bit of hassle to install server
  * VS Code needs user with VS code, and server that has not completely outdated OS, otherwise easy on server side
  * Other editors can do similar things
* Solution:
  * Most nodes JPH
  * VS Code / ... also used
* Outlook:
  * By and large good, proxies would be useful.

### Communication platform
* Requirements
  * Easy accounts for all participants
  * Intuitive usage
* Market
  * Slack / other commercial - usually restricted in free mode
  * mpi-m mattermost - easy, free for mpi-related projects
* Outlook:
  * keep as is

### Code sharing
* Requirements
  * Easy accounts for all participants
  * Some review process
* Market
  * Self-hosted gitlab - works, but a bit messy on the accounts
  * github.com - many people already have accounts
  * other cloud providers - most don't have accounts
* Solution: Github code repo w/ a bit of PR review
* Outlook
  * Train people beforehand
  * Only allow for %-style notebooks without embedded figures?

### Custom python packages
* easygems - basic functions for use of HEALPix in climate context
  * `healpix_show` for efficient plotting
  * delauney triangulation for remap
  * healpix to lat/lon
* UXArray
  * Xarray extension for unstructured grids

### Documentation
* easy.gems
* pythia

## Preparation

* Some talks on data concept in Fall 2024
* Explanatory examples on easy.gems
* All remap their datasets to HEALPix
* All convert data to zarr
* Difficulties
  * Many runs arrive last-minute
  * Lack of dedicated staff, so things only happen last-minute
  * Zoo of formats / variants

### Remapping
* Algorithms
  * NN
  * Triangles
* Tools
  * Python
  * CDO 
  * YAC / on the fly


### Zarr-ification
* Often as part of remap
* Rechunking can be hard for GRIB / stupid layouts
* eerie.cloud as fallback for IFS

### Hierarchy generation
* Generally easy
* Some issues with missing values / regional grids

### Copying data
* rclone
* globus
* difficult with last-minute data-set arrivals
* Much of data usually not used, so a bit of a waste if data can be cached on-demand

### Catalog building
* Intake0.7
* Script that merges catalogs with online catalog
* Web page with brief descriptions